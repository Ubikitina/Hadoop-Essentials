{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a68801-2e9a-40f3-a910-fe42f20616ba",
   "metadata": {},
   "source": [
    "# Parquet - Exercises\n",
    "\n",
    "**Creating a Table with a Parquet File and Pyarrow**\n",
    "\n",
    "Like other formats, Apache Parquet is also a valid file format for Hive data. In this exercise, we will convert a CSV file to Parquet and create a table in Hive. We will use the `Stars_Names.csv` file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb0a04-70a0-423a-984a-79a0e0b674ad",
   "metadata": {},
   "source": [
    "## Import the Dataset and create Pandas Dataframe\n",
    "\n",
    "The first thing we will do is import the file into a Notebook and create a Pandas DataFrame to read its content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584fe80a-915e-4e22-af7c-24dc1897506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p /media/notebooks/stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47126d0-a7f9-4e26-9ec1-7dd2fd4263f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-10 15:51:07--  https://github.com/Vega90/datasets/raw/main/Stars_Names.csv\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Vega90/datasets/main/Stars_Names.csv [following]\n",
      "--2024-11-10 15:51:08--  https://raw.githubusercontent.com/Vega90/datasets/main/Stars_Names.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16394 (16K) [text/plain]\n",
      "Saving to: ‘/media/notebooks/stars/Stars_Names.csv’\n",
      "\n",
      "/media/notebooks/st 100%[===================>]  16.01K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-11-10 15:51:08 (7.18 MB/s) - ‘/media/notebooks/stars/Stars_Names.csv’ saved [16394/16394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the csv file inside the folder\n",
    "! wget https://github.com/Vega90/datasets/raw/main/Stars_Names.csv \\\n",
    "-O /media/notebooks/stars/Stars_Names.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d31459-e104-4b58-b21a-0768ec6c457a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Constellation</th>\n",
       "      <th>Bayern Designation</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Approval Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acamar</td>\n",
       "      <td>Eridanus</td>\n",
       "      <td>θ1 Eridani A</td>\n",
       "      <td>HR 897</td>\n",
       "      <td>2016-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Achernar</td>\n",
       "      <td>Eridanus</td>\n",
       "      <td>α Eridani A</td>\n",
       "      <td>HR 472</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Achird</td>\n",
       "      <td>Cassiopeia</td>\n",
       "      <td>η Cassiopeiae A</td>\n",
       "      <td>HR 219</td>\n",
       "      <td>2017-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acrab</td>\n",
       "      <td>Scorpius</td>\n",
       "      <td>β1 Scorpii Aa</td>\n",
       "      <td>HR 5984</td>\n",
       "      <td>2016-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acrux</td>\n",
       "      <td>Crux</td>\n",
       "      <td>α Crucis Aa</td>\n",
       "      <td>HR 4730</td>\n",
       "      <td>2016-07-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name Constellation Bayern Designation Designation Approval Date\n",
       "0    Acamar      Eridanus       θ1 Eridani A      HR 897    2016-07-20\n",
       "1  Achernar      Eridanus        α Eridani A      HR 472    2016-06-30\n",
       "2    Achird    Cassiopeia    η Cassiopeiae A      HR 219    2017-09-05\n",
       "3     Acrab      Scorpius      β1 Scorpii Aa     HR 5984    2016-08-21\n",
       "4     Acrux          Crux        α Crucis Aa     HR 4730    2016-07-20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_stars = pd.read_csv('/media/notebooks/stars/Stars_Names.csv')\n",
    "df_stars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b082a7b-8ca0-4caf-9e6e-ee483af1cfc0",
   "metadata": {},
   "source": [
    "Since the column names contain spaces, we will rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24138e6-484a-464b-8f85-58f75e6a9a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                 object\n",
       "Constellation        object\n",
       "BayernDesignation    object\n",
       "Designation          object\n",
       "ApprovalDate         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "df_stars.columns = ['Name', 'Constellation', 'BayernDesignation', 'Designation', 'ApprovalDate']\n",
    "\n",
    "# Get column types, they are of type object (string)\n",
    "df_stars.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed1e2f-8a4b-4671-b20a-a167dfa5975d",
   "metadata": {},
   "source": [
    "## Convert to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdc974-97b4-4693-8cd8-1a86c25ec13e",
   "metadata": {},
   "source": [
    "Then, from this file, we will create a Parquet file that we will upload to HDFS and use to create a table in Hive. \n",
    "\n",
    "In this case, we will use the PyArrow library, which allows us to define the schema that we will use for our Parquet file. This way, we will have more control and flexibility over the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e35a7b0-950a-47db-9296-40aeb94fff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema with pyarrow\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the schema\n",
    "esquema = pa.schema([\n",
    "    ('Name', pa.string()),\n",
    "    ('Constellation', pa.string()),\n",
    "    ('BayernDesignation', pa.string()),\n",
    "    ('Designation', pa.string()),\n",
    "    ('ApprovalDate', pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6b113bd-fcdb-43a3-9461-2d4783f111e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001d",
      "J��ѣH�*]ʴ�ӧ&��\u0001\u001c",
      "\u0015\f",
      "\u00195\u0000\u0006\u0010\u0019\u0018\u000b",
      "Designation\u0015\u0002\u0016�\u0005\u0016�@\u0016�\"&��\u0001&�v\u001c",
      "6\u0000(\f",
      "PSR B1257+12\u0018\u0006GJ 551\u0000\u0019,\u0015\u0004\u0015\u0000\u0015\u0002\u0000\u0015\u0000\u0015\u0010\u0015\u0002\u0000\u0000\u0000\u0015\u0004\u0015�\u0002\u0015�\u0001L\u0015\u001a\u0015\u0000\u0012\u0000\u0000�\u00014\n",
      "\u001c",
      "\u00047-:F2-0\u00118\u00008\u0001\u001c",
      "\u0015\u000e\f",
      "6-10.b\u0000\u001011-06\u0015FH11-19\n",
      "\u0000\u0000\u00002018-08-10\u0015\u0000\u0015�\u0002\u0015�\u0002,\u0015�\u0005\u0015\u0010\u0015\u0006\u0015\u0006\u001c",
      "6\u0000(\n",
      "2018-08-10\u0018\n",
      "2015-12-15\u0000\u0000\u0000�\u0001�\u0003\u0000\u0000\u0000�\u0005\u0001\u0004U\u00102\u00000C\"RP\u0011\u0001\"2\u0013\u0010\u0000a1\"u\u0000\u0003p\u0000�7h&c1\u0007\u0000ce`\u0011�P#5\u0000Uxe\u0015aS�\u0011\u00112C$\u0000\u0004\u0007#qq3!4�0@d�\u0006�\u000b",
      "�<\u0006&�#Lcv\u0006\u0000%\u0003��;DR\u0006\u0015r\u0003\u0001%\u0010SQ\u0002\u0003 \u0006\u0011\u0002 1S@83\u0002S�\f",
      "\u0000�\u0012!B`\u00120T\u0001\u0013\u0016Z\u0003c33\u00032e1\"S0\u00105Q5Sb\u0006\u0000�S'\u0017bFLR8\u0012\u0004\u0003�b�S0V02&��\u0001\u001c",
      "\u0015\f",
      "\u00195\u0000\u0006\u0010\u0019\u0018\f",
      "ApprovalDate\u0015\u0002\u0016�\u0005\u0016�\u0006\u0016�\u0005&ڜ\u0001&ښ\u0001\u001c",
      "6\u0000(\n",
      "2018-08-10\u0018\n",
      "2015-12-15\u0000\u0019,\u0015\u0004\u0015\u0000\u0015\u0002\u0000\u0015\u0000\u0015\u0010\u0015\u0002\u0000\u0000\u0000\u0015\u0004\u0019l5\u0000\u0018\u0006schema\u0015\n",
      "Constellation\u0015\u0002\u0016�\u0005\u0016�\u0010\u0016�\u000e&�=&�3\u001c",
      "6\u0002(\tVulpecula\u0018\tAndromeda\u0000\u0019,\u0015\u0004\u0015\u0000\u0015\u0002\u0000\u0015\u0000\u0015\u0010\u0015\u0002\u0000\u0000\u0000&�t\u001c",
      "\u0015\f",
      "\u00195\u0000\u0006\u0010\u0019\u0018\u0011BayernDesignation\u0015\u0002\u0016�\u0005\u0016�]\u0016�1&�m&�C\u001c",
      "6\u0002(\u000fω Sagittarii A\u0018\u000f14 Andromedae A\u0000\u0019,\u0015\u0004\u0015\u0000\u0015\u0002\u0000\u0015\u0000\u0015\u0010\u0015\u0002\u0000\u0000\u0000&��\u0001\u001c",
      "\u0015\f",
      "\u00195\u0000\u0006\u0010\u0019\u0018\u000b",
      "Designation\u0015\u0002\u0016�\u0005\u0016�@\u0016�\"&��\u0001&�v\u001c",
      "6\u0000(\f",
      "PSR B1257+12\u0018\u0006GJ 551\u0000\u0019,\u0015\u0004\u0015\u0000\u0015\u0002\u0000\u0015\u0000\u0015\u0010\u0015\u0002\u0000\u0000\u0000&��\u0001\u001c",
      "\u0015\f",
      "\u00195\u0000\u0006\u0010\u0019\u0018\f",
      "ApprovalDate\u0015\u0002\u0016�\u0005\u0016�\u0006\u0016�\u0005&ڜ\u0001&ښ\u0001\u001c",
      "6\u0000(\n",
      "2018-08-10\u0018\n",
      "/////+AEAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABBAAQAAAAAAAKAAwAAAAEAAgACgAAAKADAAAEAAAAAQAAAAwAAAAIAAwABAAIAAgAAAB4AwAABAAAAGgDAAB7ImluZGV4X2NvbHVtbnMiOiBbXSwgImNvbHVtbl9pbmRleGVzIjogW3sibmFtZSI6IG51bGwsICJmaWVsZF9uYW1lIjogbnVsbCwgInBhbmRhc190eXBlIjogInVuaWNvZGUiLCAibnVtcHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiB7ImVuY29kaW5nIjogIlVURi04In19XSwgImNvbHVtbnMiOiBbeyJuYW1lIjogIk5hbWUiLCAiZmllbGRfbmFtZSI6ICJOYW1lIiwgInBhbmRhc190eXBlIjogInVuaWNvZGUiLCAibnVtcHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJuYW1lIjogIkNvbnN0ZWxsYXRpb24iLCAiZmllbGRfbmFtZSI6ICJDb25zdGVsbGF0aW9uIiwgInBhbmRhc190eXBlIjogInVuaWNvZGUiLCAibnVtcHlfdHlwZSI6ICJvYmplY3QiLCAibWV0YWRhdGEiOiBudWxsfSwgeyJuYW1lIjogIkJheWVybkRlc2lnbmF0aW9uIiwgImZpZWxkX25hbWUiOiAiQmF5ZXJuRGVzaWduYXRpb24iLCAicGFuZGFzX3R5cGUiOiAidW5pY29kZSIsICJudW1weV90eXBlIjogIm9iamVjdCIsICJtZXRhZGF0YSI6IG51bGx9LCB7Im5hbWUiOiAiRGVzaWduYXRpb24iLCAiZmllbGRfbmFtZSI6ICJEZXNpZ25hdGlvbiIsICJwYW5kYXNfdHlwZSI6ICJ1bmljb2RlIiwgIm51bXB5X3R5cGUiOiAib2JqZWN0IiwgIm1ldGFkYXRhIjogbnVsbH0sIHsibmFtZSI6ICJBcHByb3ZhbERhdGUiLCAiZmllbGRfbmFtZSI6ICJBcHByb3ZhbERhdGUiLCAicGFuZGFzX3R5cGUiOiAidW5pY29kZSIsICJudW1weV90eXBlIjogIm9iamVjdCIsICJtZXRhZGF0YSI6IG51bGx9XSwgImNyZWF0b3IiOiB7ImxpYnJhcnkiOiAicHlhcnJvdyIsICJ2ZXJzaW9uIjogIjE3LjAuMCJ9LCAicGFuZGFzX3ZlcnNpb24iOiAiMi4yLjIifQAAAAAGAAAAcGFuZGFzAAAFAAAA5AAAAKAAAABoAAAAOAAAAAQAAABA////AAABBRAAAAAgAAAABAAAAAAAAAAMAAAAQXBwcm92YWxEYXRlAAAAADj///9w////AAABBRAAAAAcAAAABAAAAAAAAAALAAAARGVzaWduYXRpb24AZP///5z///8AAAEFEAAAACQAAAAEAAAAAAAAABEAAABCYXllcm5EZXNpZ25hdGlvbgAAAJj////Q////AAABBRAAAAAgAAAABAAAAAAAAAANAAAAQ29uc3RlbGxhdGlvbgAAAMj///8QABQACAAGAAcADAAAABAAEAAAAAAAAQUQAAAAHAAAAAQAAAAAAAAABAAAAE5hbWUAAAAABAAEAAQAAAA=\u0000\u0018 parquet-cpp-arrow version 17.0.0\u0019\\\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u001c",
      "\u0000\u0000\u0000�\f",
      "\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "# Convert the pandas DataFrame to a PyArrow table with the defined schema\n",
    "table = pa.Table.from_pandas(df_stars, schema=esquema)\n",
    "\n",
    "# Convert to Parquet\n",
    "pq.write_table(table, '/media/notebooks/stars/stars.parq')\n",
    "\n",
    "# Display parquet file\n",
    "! cat /media/notebooks/stars/stars.parq | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46730642-6f74-4292-8043-71185096c4ea",
   "metadata": {},
   "source": [
    "Once again, we compare the size of both files and see that the Parquet file is smaller than the CSV file, as in the previous example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f629659-25db-4e5f-b6ef-86131c0964d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the CSV file is:\n",
      "16394\n",
      "The size of the PARQUET file is:\n",
      "13604\n"
     ]
    }
   ],
   "source": [
    "print('The size of the CSV file is:')\n",
    "! stat -c%s /media/notebooks/stars/Stars_Names.csv\n",
    "\n",
    "# tamaño del fichero parquet\n",
    "print('The size of the PARQUET file is:')\n",
    "! stat -c%s /media/notebooks/stars/stars.parq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43001561-eac5-4b3e-a5d8-48913cddea1b",
   "metadata": {},
   "source": [
    "## Creating the Stars Table (Hive) from the Parquet File\n",
    "\n",
    "We need to upload the file we just created to HDFS. We can upload it to `/stars-parquet`, for example. Once done, we will create an external table called `Stars`, but this time we will use the Parquet file we just created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2238f178-a119-4df0-8109-8a504b0a29dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Constellation</th>\n",
       "      <th>BayernDesignation</th>\n",
       "      <th>Designation</th>\n",
       "      <th>ApprovalDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acamar</td>\n",
       "      <td>Eridanus</td>\n",
       "      <td>θ1 Eridani A</td>\n",
       "      <td>HR 897</td>\n",
       "      <td>2016-07-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Achernar</td>\n",
       "      <td>Eridanus</td>\n",
       "      <td>α Eridani A</td>\n",
       "      <td>HR 472</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Achird</td>\n",
       "      <td>Cassiopeia</td>\n",
       "      <td>η Cassiopeiae A</td>\n",
       "      <td>HR 219</td>\n",
       "      <td>2017-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acrab</td>\n",
       "      <td>Scorpius</td>\n",
       "      <td>β1 Scorpii Aa</td>\n",
       "      <td>HR 5984</td>\n",
       "      <td>2016-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acrux</td>\n",
       "      <td>Crux</td>\n",
       "      <td>α Crucis Aa</td>\n",
       "      <td>HR 4730</td>\n",
       "      <td>2016-07-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name Constellation BayernDesignation Designation ApprovalDate\n",
       "0    Acamar      Eridanus      θ1 Eridani A      HR 897   2016-07-20\n",
       "1  Achernar      Eridanus       α Eridani A      HR 472   2016-06-30\n",
       "2    Achird    Cassiopeia   η Cassiopeiae A      HR 219   2017-09-05\n",
       "3     Acrab      Scorpius     β1 Scorpii Aa     HR 5984   2016-08-21\n",
       "4     Acrux          Crux       α Crucis Aa     HR 4730   2016-07-20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataFrame from the parquet file\n",
    "dt_parquet = pd.read_parquet('/media/notebooks/stars/stars.parq')\n",
    "dt_parquet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8989af50-d3a0-4937-897e-6d664e87327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to HDFS\n",
    "! hdfs dfs -mkdir /stars-parquet\n",
    "! hdfs dfs -put /media/notebooks/stars/stars.parq /stars-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea35bc3-3723-483d-abe5-8fbb3d1c7c22",
   "metadata": {},
   "source": [
    "Create a external table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63d175bf-3f0f-4018-baac-816afef67500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 9594958e-9f1b-4e8d-9379-38bae83fb575\n",
      "24/11/10 16:00:41 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/10 16:00:44 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:45 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:00:46 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No rows affected (1.489 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/10 16:00:48 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/10 16:00:48 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"CREATE EXTERNAL TABLE starsparquet( \\\n",
    "                 Name STRING, \\\n",
    "                 Constellation STRING, \\\n",
    "                 BayernDesignation STRING, \\\n",
    "                 Designation STRING, \\\n",
    "                 ApprovalDate STRING \\\n",
    "            ) \\\n",
    "            STORED AS PARQUET \\\n",
    "            LOCATION '/stars-parquet';\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920cd60-7f8f-4ed8-893e-fa4efbdb1260",
   "metadata": {},
   "source": [
    "The previous command will create an external table that has the data in that location, but in Parquet format. Now, we don’t need to specify the delimiter or indicate that our data has headers. If we run this command in Hive, we can create the table and perform queries as we did in previous modules. For example, we can retrieve the first three records from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aa7ca0-ec77-49ed-86ef-480b9670e169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 9aa72454-9416-43a0-ac54-5ee83cfe92ba\n",
      "24/11/10 16:01:11 [main]: WARN hikari.HikariConfig: objectstore - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/10 16:01:11 [main]: WARN hikari.HikariConfig: objectstore-secondary - leakDetectionThreshold is less than 2000ms or more than maxLifetime, disabling it.\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:12 [main]: WARN DataNucleus.MetaData: Metadata has jdbc-type of null yet this is not valid. Ignored\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.cpc.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.hll.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.IntersectSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.EstimateSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.ExcludeSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.theta.UnionSketchUDF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN exec.FunctionRegistry: UDF Class org.apache.hive.org.apache.datasketches.hive.tuple.ArrayOfDoublesSketchToValuesUDTF does not have description. Please annotate the class with the org.apache.hadoop.hive.ql.exec.Description annotation and provide the description of the function.\n",
      "24/11/10 16:01:13 [main]: WARN session.SessionState: Configuration hive.reloadable.aux.jars.path not specified\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "24/11/10 16:01:15 [3c9582c6-6e20-46ac-8138-9810e4710ae8 main]: WARN calcite.RelOptHiveTable: No Stats for default@starsparquet, Columns: approvaldate, constellation, name, designation, bayerndesignation\n",
      "No Stats for default@starsparquet, Columns: approvaldate, constellation, name, designation, bayerndesignation\n",
      "24/11/10 16:01:16 [3c9582c6-6e20-46ac-8138-9810e4710ae8 main]: WARN optimizer.SimpleFetchOptimizer: Table default@starsparquet is external table, falling back to filesystem scan.\n",
      "+--------------------+-----------------------------+---------------------------------+---------------------------+----------------------------+\n",
      "| starsparquet.name  | starsparquet.constellation  | starsparquet.bayerndesignation  | starsparquet.designation  | starsparquet.approvaldate  |\n",
      "+--------------------+-----------------------------+---------------------------------+---------------------------+----------------------------+\n",
      "| Acamar             | Eridanus                    | ?1 Eridani A                    | HR 897                    | 2016-07-20                 |\n",
      "| Achernar           | Eridanus                    | ? Eridani A                     | HR 472                    | 2016-06-30                 |\n",
      "| Achird             | Cassiopeia                  | ? Cassiopeiae A                 | HR 219                    | 2017-09-05                 |\n",
      "+--------------------+-----------------------------+---------------------------------+---------------------------+----------------------------+\n",
      "3 rows selected (3.28 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n",
      "24/11/10 16:01:16 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n",
      "24/11/10 16:01:16 [shutdown-hook-0]: WARN util.ShutdownHookManager: Shutdown in progress, cannot cancel a deleteOnExit\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT * FROM starsparquet LIMIT 3;\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
