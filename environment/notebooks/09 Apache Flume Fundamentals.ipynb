{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Apache Flume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folders\n",
    "\n",
    "- First, we will create a folder named \"flume\" in `/media/notebooks/` to store all our work with Flume.\n",
    "- Inside the \"flume\" folder, we will create another folder named \"configuraciones\" to hold all our Flume configuration files.\n",
    "- We will also create a \"data\" folder, where we will store the files to be read and later sent via Flume to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p88oijhI8f9N"
   },
   "outputs": [],
   "source": [
    "# Create folders\n",
    "! mkdir -p flume\n",
    "! mkdir -p flume/data\n",
    "! mkdir -p flume/configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5mGVLqz28f9P",
    "outputId": "aadd040e-1050-4241-b793-0edc081fe971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/flume/configuraciones\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/flume/configuraciones\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5PrmQMy87mg"
   },
   "source": [
    "## Agent Configuration File\n",
    "\n",
    "The configuration file outlines the data flow, which involves copying the content of files in a specified local path to a temporary HDFS file that Flume will create for us. This setup allows an agent to \"listen\" to everything that arrives in a folder and establish a data flow from that folder to HDFS. Here’s the configuration breakdown:\n",
    "\n",
    "- **General Agent Configuration**: Here, we name the agent as \"agent1.\" In the initial lines, we define the names for the source, channel, and sink, which will be `source1`, `sink1`, and `channel1`.\n",
    "\n",
    "- **Source Configuration**: We specify the type of source, which in this case is `spooldir`, since we’ll be monitoring a local directory of files. We then set the path for this source.\n",
    "\n",
    "- **Channel Configuration**: We define the channel type that will connect our source with the sink. For this setup, we use `memory`, a memory channel that stores events in the agent’s memory.\n",
    "\n",
    "- **Sink Configuration**: Since we’ll be writing to HDFS, the sink type is set to `hdfs`, and we provide the HDFS path where the files will be stored. We then specify the output file type as `DataStream` and the data format as `Text`.\n",
    "\n",
    "- **Linking Source and Sink to the Channel**: We specify the channel used by the source and sink. Notably, for the source, we use the `channels` property, while for the sink, we use `channel`. This is because a source can write to multiple channels, while a sink can only read from one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hy3amae48f9R",
    "outputId": "a875d770-875f-4633-8b4b-3c33eefe8176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ejemplo.config\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo.config\n",
    "# Configuracion nombres componentes\n",
    "agente1.sources = source1\n",
    "agente1.sinks = sink1\n",
    "agente1.channels = channel1\n",
    "\n",
    "# Configuración source\n",
    "agente1.sources.source1.type = spooldir\n",
    "agente1.sources.source1.spoolDir = /media/notebooks/flume/data/ejemplo\n",
    "\n",
    "# Configuración channel\n",
    "agente1.channels.channel1.type = memory\n",
    "\n",
    "# Configuración sink\n",
    "agente1.sinks.sink1.type = hdfs\n",
    "agente1.sinks.sink1.hdfs.path = hdfs://namenode:9000/ejemplo-flume/\n",
    "agente1.sinks.sink1.hdfs.fileType = DataStream\n",
    "agente1.sinks.sink1.hdfs.writeFormat = Text\n",
    "\n",
    "# Vinculación de source y sink con channel\n",
    "agente1.sources.source1.channels = channel1\n",
    "agente1.sinks.sink1.channel = channel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QQKSjVb58f9R",
    "outputId": "5037047a-7e76-43dd-c45c-17eee1e39587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/flume/data/ejemplo\n"
     ]
    }
   ],
   "source": [
    "#creamos la carpeta ejemplo1 para los ficheros de entrada\n",
    "! mkdir -p /media/notebooks/flume/data/ejemplo\n",
    "os.chdir(\"/media/notebooks/flume/data/ejemplo\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YvE6u4f9BC4"
   },
   "source": [
    "## Create Dataset\n",
    "\n",
    "We create the three files from which we will read their contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L1AhrP9H8f9T",
    "outputId": "5340314e-8002-4b43-c5e7-4ebb09948c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ejemplo1-fichero1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo1-fichero1.txt\n",
    "Este es el texto del fichero 1 para el ejemplo de flume agente1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PH1YKibj8f9T",
    "outputId": "16ac43ba-b5a1-4cc4-e098-a2fd6a939994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ejemplo1-fichero2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo1-fichero2.txt\n",
    "Sin embargo, este es el texto del fichero 2 para el ejemplo de flume agente1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8jQdHDaJ8f9U",
    "outputId": "79da6793-ae59-4408-b7d2-b197bf8ce683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ejemplo1-fichero3.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ejemplo1-fichero3.txt\n",
    "Por ultimo, este es el texto del fichero 3 para el ejemplo de flume agente1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX2ijogL8f9U",
    "outputId": "eebeb8d6-dfbb-44eb-b737-0f433f389a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplo1-fichero1.txt  ejemplo1-fichero2.txt  ejemplo1-fichero3.txt\r\n"
     ]
    }
   ],
   "source": [
    "# List the content of the folder\n",
    "!ls /media/notebooks/flume/data/ejemplo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0O7YUji98f9V",
    "outputId": "af0c3690-22c9-4e70-f8e6-cd72c12d55ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplo1-fichero1.txt  ejemplo1-fichero2.txt  ejemplo1-fichero3.txt\n"
     ]
    }
   ],
   "source": [
    "# List the content of the folder\n",
    "!ls /media/notebooks/flume/data/ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urEKRbnk9UGG"
   },
   "source": [
    "## Create the Agent and Execute it\n",
    "\n",
    "Start an Apache Flume agent named `agente1`, using a specific configuration located in `ejemplo.config`, with the log level in `INFO` displayed in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XWCf0Jsd8f9W",
    "outputId": "f218f880-46e4-45bf-97d3-0e0fd9c3a22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Including Hadoop libraries found via (/usr/local/hadoop/bin/hadoop) for HDFS access\n",
      "Info: Including Hive libraries found via (/usr/local/hive) for Hive access\n",
      "+ exec /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp './conf/:/usr/local/flume/lib/*:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hive/lib/*' -Djava.library.path=:/usr/local/hadoop/lib/native org.apache.flume.node.Application --conf-file /media/notebooks/flume/configuraciones/ejemplo.config --name agente1\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/flume/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.18.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! flume-ng agent --conf ./conf/ \\\n",
    "    --conf-file /media/notebooks/flume/configuraciones/ejemplo.config \\\n",
    "    --name agente1 \\\n",
    "    -Dflume.root.logger=INFO,console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTLRzZL_8f9W"
   },
   "source": [
    "We must wait until all the files are read and then stop the execution of the agent (stopping the Jupyter kernel with the stop button or with ctrl+c in the terminal) once we see that the reading of the files has been completed correctly. Since Flume receives the data in streaming, it does not know when to stop listening, therefore, if we do not stop it manually, it will be active indefinitely and we will not be able to execute the following cells.\n",
    "\n",
    "How to see that the reading of the files has been completed correctly: the word COMPLETED has been added to the end of the file name in the folder `/flume/data/ejemplo1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON1Bm92D9ZyO"
   },
   "source": [
    "See the content of the source folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6T9t1TPk8f9X",
    "outputId": "d30919f0-2dbc-45be-cbe6-ad7904f1b090"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplo1-fichero1.txt.COMPLETED  ejemplo1-fichero3.txt.COMPLETED\n",
      "ejemplo1-fichero2.txt.COMPLETED\n"
     ]
    }
   ],
   "source": [
    "!ls /media/notebooks/flume/data/ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qJtlgNj9duQ"
   },
   "source": [
    "See the content of the HDFS folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cjLZKuHT8f9Y",
    "outputId": "974cb734-4f6e-4891-821e-a9a1b96b7c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup        217 2024-08-26 13:55 /ejemplo-flume/FlumeData.1724680541208\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /ejemplo-flume/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Flume file has been created by the agent reading the content of the file, sending it through the channel, in this case the agent, and finally it has been written by the sink in HDFS.\n",
    "\n",
    "We can see the content of the file and we can see, indeed, how everything has been transmitted successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ne6AWiWJ8f9Y",
    "outputId": "06318137-47bc-422c-d95f-665c2e3e6e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este es el texto del fichero 1 para el ejemplo de flume agente1\n",
      "Sin embargo, este es el texto del fichero 2 para el ejemplo de flume agente1\n",
      "Por ultimo, este es el texto del fichero 3 para el ejemplo de flume agente1\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /ejemplo-flume/FlumeData*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
