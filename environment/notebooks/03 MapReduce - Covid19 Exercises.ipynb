{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadb5de0-bcb0-4d96-9d92-179d20fd4804",
   "metadata": {},
   "source": [
    "# Covid19 MapReduce Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68920054-19a5-452f-a31a-8b6ffedd5114",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "This data file contains information about COVID-19 from January 21, 2020, to May 13, 2022, across various counties in the United States. The data was collected by *The New York Times*, based on reports from state and local health agencies. The original file can be found on Kaggle: *NY-TIMES COVID-19 USA dataset*, and it is distributed under a Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC) license.\n",
    "\n",
    "The file we are using is a modified version of the original, as preprocessing was done to remove records with null values, and one column (FIPS) was removed for practical simplification, as it did not provide relevant information for the dataset.\n",
    "\n",
    "There is a total of 2,445,227 records, and the fields are as follows:\n",
    "\n",
    "- **date**: The date the data was recorded.\n",
    "- **county**: The county where cases and deaths were recorded.\n",
    "- **state**: The state where the county is located.\n",
    "- **cases**: The number of confirmed cases.\n",
    "- **deaths**: The number of confirmed deaths.\n",
    "\n",
    "Link: [https://github.com/Vega90/datasets/raw/main/covid19-NY.zip](https://github.com/Vega90/datasets/raw/main/covid19-NY.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3f996-d2e6-46db-b5b8-f53f2409b305",
   "metadata": {},
   "source": [
    "### Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9f649e-a242-4a37-a320-cdc52c161661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-28 11:49:16--  https://github.com/Vega90/datasets/raw/main/covid19-NY.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/Vega90/datasets/main/covid19-NY.zip [following]\n",
      "--2024-08-28 11:49:16--  https://raw.githubusercontent.com/Vega90/datasets/main/covid19-NY.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 24556797 (23M) [application/zip]\n",
      "Saving to: ‘/media/notebooks/covid19/covid19-NY.zip’\n",
      "\n",
      "/media/notebooks/co 100%[===================>]  23.42M  18.6MB/s    in 1.3s    \n",
      "\n",
      "2024-08-28 11:49:20 (18.6 MB/s) - ‘/media/notebooks/covid19/covid19-NY.zip’ saved [24556797/24556797]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# Create a folder to save the file\n",
    "! mkdir -p /media/notebooks/covid19\n",
    "\n",
    "# Download the file and save it\n",
    "! wget https://github.com/Vega90/datasets/raw/main/covid19-NY.zip \\\n",
    "-O /media/notebooks/covid19/covid19-NY.zip\n",
    "\n",
    "# Unzip the file\n",
    "zip = ZipFile('/media/notebooks/covid19/covid19-NY.zip')\n",
    "zip.extractall('/media/notebooks/covid19/')\n",
    "zip.close()\n",
    "\n",
    "# Delete the zip file\n",
    "! rm /media/notebooks/covid19/covid19-NY.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4bb4d-1ddf-4a18-82e5-5da2b5742fb4",
   "metadata": {},
   "source": [
    "### Upload it to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc2d15a-f4fc-40f7-b5e1-bf699baf346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup   90199922 2024-08-28 11:49 /covid19/covid19-NY.csv\n"
     ]
    }
   ],
   "source": [
    "# Upload the file to HDFS \n",
    "! hdfs dfs -mkdir /covid19\n",
    "! hdfs dfs -put /media/notebooks/covid19/covid19-NY.csv /covid19\n",
    "! hdfs dfs -ls /covid19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd43f4e-db44-4851-ac80-35ad773adfd8",
   "metadata": {},
   "source": [
    "### Create the folder for the exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ceb592b-9c66-4536-a72f-4f7c6ec48813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/covid19/mapreduce\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create the folder for the exercises\n",
    "! mkdir -p /media/notebooks/covid19/mapreduce\n",
    "\n",
    "os.chdir(\"/media/notebooks/covid19/mapreduce\")\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f8f48-0425-4f58-bffd-7a81c5b98a49",
   "metadata": {},
   "source": [
    "### Pandas dataframe to help us understand the data\n",
    "\n",
    "We will use a pandas dataframe to help us understand the data and we can also check the exercises by comparing the given output and the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2472d622-44d7-4325-8c97-64030554684c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Cook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     county       state  cases  deaths\n",
       "0  2020-01-21  Snohomish  Washington      1       0\n",
       "1  2020-01-22  Snohomish  Washington      1       0\n",
       "2  2020-01-23  Snohomish  Washington      1       0\n",
       "3  2020-01-24       Cook    Illinois      1       0\n",
       "4  2020-01-24  Snohomish  Washington      1       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data columns\n",
    "columnas = ['date', 'county', 'state', 'cases', 'deaths']\n",
    "\n",
    "# Read the CSV to a dataframe\n",
    "dataframe = pd.read_csv('/media/notebooks/covid19/covid19-NY.csv', names=columnas)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584c178-e9bd-4145-8945-2729e142928f",
   "metadata": {},
   "source": [
    "## Exercise 1 - Counties Counter\n",
    "\n",
    "The first exercise is to count the number of unique values in the county column. That way, we will know exactly how many counties we have data on the Coronavirus cases we are going to study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45727ff8-97be-4c0a-b011-fff05586f88d",
   "metadata": {},
   "source": [
    "**MAP Process # 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcfafaa-41ca-478c-af89-8e5293941be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 01_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 01_mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Standard input STDIN\n",
    "for linea in sys.stdin:\n",
    "  # Delete spaces\n",
    "  fila = linea.strip()\n",
    "  # Divide each line in columns\n",
    "  campos = fila.split(',')\n",
    "\n",
    "  # Write each county to be the input of the reducer\n",
    "  condado = campos[1]\n",
    "  print(f\"{condado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549eb29-070d-4882-b34a-36fcccf92445",
   "metadata": {},
   "source": [
    "**REDUCE Process # 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "454af3c3-94ed-41d6-a304-9c256f00836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 01_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 01_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "  \n",
    "# We use a set to avoid duplicates\n",
    "condados = {linea.strip() for linea in sys.stdin}\n",
    "\n",
    "# Total number of counties\n",
    "print(f\"Total number of counties:\\t{len(condados)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f8c512-4b4c-4e9b-8db3-10a88ca0af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero total de condados:\t1856\n"
     ]
    }
   ],
   "source": [
    "# Run map-reduce and display the result to make sure that the files are correctly defined\n",
    "! cat /media/notebooks/covid19/covid19-NY.csv \\\n",
    "| python3 01_mapper.py | sort | python3 01_reducer.py > salida-01\n",
    "\n",
    "! cat salida-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbbe5b-bb5b-4c85-90e6-bf8e6c6a3b74",
   "metadata": {},
   "source": [
    "**Hadoop Streaming Execution # 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7d93b5-9714-4485-85df-b4632be741f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar3530511188227510689/] [] /tmp/streamjob6913980281073132578.jar tmpDir=null\n",
      "2024-08-28 11:50:03,080 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:50:03,360 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:50:03,897 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1724833657149_0030\n",
      "2024-08-28 11:50:05,289 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-08-28 11:50:05,741 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-08-28 11:50:07,134 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1724833657149_0030\n",
      "2024-08-28 11:50:07,134 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-08-28 11:50:07,962 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-08-28 11:50:07,963 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-08-28 11:50:08,274 INFO impl.YarnClientImpl: Submitted application application_1724833657149_0030\n",
      "2024-08-28 11:50:08,359 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1724833657149_0030/\n",
      "2024-08-28 11:50:08,365 INFO mapreduce.Job: Running job: job_1724833657149_0030\n",
      "2024-08-28 11:50:23,832 INFO mapreduce.Job: Job job_1724833657149_0030 running in uber mode : false\n",
      "2024-08-28 11:50:23,833 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-08-28 11:50:41,603 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-08-28 11:50:53,945 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-08-28 11:50:56,023 INFO mapreduce.Job: Job job_1724833657149_0030 completed successfully\n",
      "2024-08-28 11:50:56,210 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=27330951\n",
      "\t\tFILE: Number of bytes written=55604480\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=90204208\n",
      "\t\tHDFS: Number of bytes written=31\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29865\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9411\n",
      "\t\tTotal time spent by all map tasks (ms)=29865\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9411\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=29865\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=9411\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=30581760\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=9636864\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2445227\n",
      "\t\tMap output records=2445227\n",
      "\t\tMap output bytes=22440491\n",
      "\t\tMap output materialized bytes=27330957\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1856\n",
      "\t\tReduce shuffle bytes=27330957\n",
      "\t\tReduce input records=2445227\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=4890454\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=788\n",
      "\t\tCPU time spent (ms)=14960\n",
      "\t\tPhysical memory (bytes) snapshot=1038053376\n",
      "\t\tVirtual memory (bytes) snapshot=7637102592\n",
      "\t\tTotal committed heap usage (bytes)=879230976\n",
      "\t\tPeak Map Physical memory (bytes)=356372480\n",
      "\t\tPeak Map Virtual memory (bytes)=2542374912\n",
      "\t\tPeak Reduce Physical memory (bytes)=327127040\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2552799232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90204018\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=31\n",
      "2024-08-28 11:50:56,211 INFO streaming.StreamJob: Output directory: /covid19/mapreduce/01_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 01_mapper.py,01_reducer.py \\\n",
    "-mapper 01_mapper.py \\\n",
    "-reducer 01_reducer.py \\\n",
    "-input /covid19/covid19-NY.csv \\\n",
    "-output /covid19/mapreduce/01_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befb0e3a-6c85-4268-a7b6-569f1fe55091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero total de condados:\t1856\n",
      "\n",
      "Valores en el dataframe:\t1856\n"
     ]
    }
   ],
   "source": [
    "# Display the output of exercise 1\n",
    "! hdfs dfs -cat /covid19/mapreduce/01_salida/part-00000\n",
    "\n",
    "# Check the number of unique values for that column in the dataframe \n",
    "num = dataframe['county'].nunique()\n",
    "print(f\"\\nValores en el dataframe:\\t{num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1670ea8-b2a4-489c-95f1-9a12cd34f59f",
   "metadata": {},
   "source": [
    "## Exercise 2- Search for the day with the highest number of fatalities\n",
    "\n",
    "This second exercise consists of obtaining the day on which the greatest number of deaths was recorded. To do this we must program two phases:\n",
    "1. Obtain the value of deaths and the day.\n",
    "2. Add the partial results grouped by day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f957977-64ad-4e47-affa-89aca54b9fef",
   "metadata": {},
   "source": [
    "**MAP Process # 02**\n",
    "\n",
    "Processes CSV input line by line, extracts the date and the number of deaths, handles any missing values by assigning a default of 0, and then outputs the results as key-value pairs with the date as the key and the number of deaths as the value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ee8f0a-72c3-44dc-8719-aef15cff38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 02_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 02_mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Standard Input STDIN\n",
    "for linea in sys.stdin:\n",
    "    \n",
    "  fila = linea.strip()\n",
    "  # Divide each line in columns\n",
    "  campos = fila.split(',')\n",
    "\n",
    "  # Keep the fields we want\n",
    "  fecha = campos[0]\n",
    "  muertes = campos[4]\n",
    "\n",
    "  if muertes == None:\n",
    "      muertes = 0;\n",
    "\n",
    "  # Write the fields to be the input of the reducer        \n",
    "  print(f\"{fecha}\\t{muertes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9131ce-1767-4cee-b1f6-d9c1d79236a3",
   "metadata": {},
   "source": [
    "**Reduce Process # 02**\n",
    "\n",
    "Processes the key-value pairs outputted by the Mapper (date and number of deaths), accumulates the total deaths for each date, and then calculates the day with the highest number of deaths. It finally prints the date and the total number of deaths for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10cb42a-1c51-4391-8720-203675839979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 02_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 02_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# Dictionary for the date and number of deaths\n",
    "muertes_dia = defaultdict(int)\n",
    "\n",
    "# Process each input line\n",
    "for linea in sys.stdin:\n",
    "    fecha, muertes = linea.strip().split('\\t')\n",
    "    muertes_dia[fecha] += int(muertes)\n",
    "\n",
    "# Calculate the day with the highest number of deaths\n",
    "max_dia, max_muertes = max(muertes_dia.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"El dia con mayor numero de muertes fue el {max_dia}\")\n",
    "print(f\"Hubo un total de {max_muertes} muertes\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e050d7fb-2295-42c5-934f-582a6ab193da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dia con mayor numero de muertes fue el 2022-05-13\n",
      "Hubo un total de 998279 muertes\n"
     ]
    }
   ],
   "source": [
    "# Run map-reduce from the console\n",
    "! cat /media/notebooks/covid19/covid19-NY.csv \\\n",
    "| python3 02_mapper.py | sort | python3 02_reducer.py > salida-02\n",
    "\n",
    "! cat salida-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499284e-915a-4868-a1b5-f17e82fdc1e2",
   "metadata": {},
   "source": [
    "**Hadoop Streaming Execution # 02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802f8212-767f-40c6-9c90-89f014b5d520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar5085335737554451389/] [] /tmp/streamjob6791996230282016498.jar tmpDir=null\n",
      "2024-08-28 11:51:11,800 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:51:12,074 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:51:12,555 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1724833657149_0031\n",
      "2024-08-28 11:51:13,480 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-08-28 11:51:13,771 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-08-28 11:51:14,214 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1724833657149_0031\n",
      "2024-08-28 11:51:14,214 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-08-28 11:51:14,683 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-08-28 11:51:14,684 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-08-28 11:51:14,843 INFO impl.YarnClientImpl: Submitted application application_1724833657149_0031\n",
      "2024-08-28 11:51:14,902 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1724833657149_0031/\n",
      "2024-08-28 11:51:14,909 INFO mapreduce.Job: Running job: job_1724833657149_0031\n",
      "2024-08-28 11:51:24,317 INFO mapreduce.Job: Job job_1724833657149_0031 running in uber mode : false\n",
      "2024-08-28 11:51:24,319 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-08-28 11:51:44,518 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2024-08-28 11:51:49,057 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-08-28 11:52:22,293 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "2024-08-28 11:52:25,431 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-08-28 11:52:26,514 INFO mapreduce.Job: Job job_1724833657149_0031 completed successfully\n",
      "2024-08-28 11:52:26,773 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=39091968\n",
      "\t\tFILE: Number of bytes written=79126514\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=90204208\n",
      "\t\tHDFS: Number of bytes written=87\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=44524\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=31034\n",
      "\t\tTotal time spent by all map tasks (ms)=44524\n",
      "\t\tTotal time spent by all reduce tasks (ms)=31034\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=44524\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=31034\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=45592576\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=31778816\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2445227\n",
      "\t\tMap output records=2445227\n",
      "\t\tMap output bytes=34201508\n",
      "\t\tMap output materialized bytes=39091974\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=844\n",
      "\t\tReduce shuffle bytes=39091974\n",
      "\t\tReduce input records=2445227\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4890454\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=876\n",
      "\t\tCPU time spent (ms)=19390\n",
      "\t\tPhysical memory (bytes) snapshot=1022373888\n",
      "\t\tVirtual memory (bytes) snapshot=7633747968\n",
      "\t\tTotal committed heap usage (bytes)=1040711680\n",
      "\t\tPeak Map Physical memory (bytes)=344281088\n",
      "\t\tPeak Map Virtual memory (bytes)=2542080000\n",
      "\t\tPeak Reduce Physical memory (bytes)=337506304\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2562285568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90204018\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=87\n",
      "2024-08-28 11:52:26,774 INFO streaming.StreamJob: Output directory: /covid19/mapreduce/02_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 02_mapper.py,02_reducer.py \\\n",
    "-mapper 02_mapper.py \\\n",
    "-reducer 02_reducer.py \\\n",
    "-input /covid19/covid19-NY.csv \\\n",
    "-output /covid19/mapreduce/02_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7782b311-1f27-4413-8885-be5764e98137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dia con mayor numero de muertes fue el 2022-05-13\t\n",
      "Hubo un total de 998279 muertes\t\n",
      "\n",
      "Salida del dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>2022-05-13</td>\n",
       "      <td>998279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  deaths\n",
       "843  2022-05-13  998279"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the output of exercise 2\n",
    "! hdfs dfs -cat /covid19/mapreduce/02_salida/part-00000 \n",
    "\n",
    "# Check the result with the dataframe\n",
    "df_agrupado = dataframe[['date', 'deaths']].groupby('date').sum().reset_index()\n",
    "max_deaths = df_agrupado[df_agrupado['deaths'] == df_agrupado['deaths'].max()]\n",
    "print(\"\\nSalida del dataframe:\\n\")\n",
    "max_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f6c6e-2c55-425a-b20c-336cc8753c2b",
   "metadata": {},
   "source": [
    "## Exercise 3 - State with the highest average number of deaths due to positive cases\n",
    "\n",
    "In this exercise we are going to extract the result of the deaths by positive cases. This way we will be able to know which is the state with the most deaths by positive cases. Again we will need the 'state' field and this time, we will use the 'cases' field in addition to the previous 'deaths' field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6711a02-63d5-4262-a0a0-1c7d8468a68a",
   "metadata": {},
   "source": [
    "**MAP Process # 03**\n",
    "\n",
    "Processes input data line by line from the standard input (STDIN). It strips any leading or trailing whitespace, splits each line into columns by commas, and extracts three specific fields: `estado` (state), `casos` (cases), and `muertes` (deaths). It then outputs these fields in the format `estado\\tcasos\\tmuertes`, which will serve as input for the **Reducer** in a MapReduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4405a1fd-f37e-4ad9-9835-3f8821036a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 03_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 03_mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# STDIN\n",
    "for linea in sys.stdin:\n",
    "  fila = linea.strip()\n",
    "  campos = fila.split(',')\n",
    "\n",
    "  estado = campos[2]\n",
    "  casos = campos[3]\n",
    "  muertes = campos[4]\n",
    "\n",
    "  # Write the fields to be the input of the reducer          \n",
    "  print(f\"{estado}\\t{casos}\\t{muertes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab24c3-d3e5-4b76-ad79-87412f188dab",
   "metadata": {},
   "source": [
    "**REDUCE Process # 03**\n",
    "\n",
    "Processes input data line by line, which contains information about states, cases, and deaths. It uses a dictionary to track the total deaths and cases for each state. For each line of input, it splits the data by tab (`\\t`) and updates the death and case counts for the respective state. After processing all the input, the script calculates the state with the highest mortality rate by dividing the total deaths by the total cases for each state. It then prints the state with the highest mortality rate and the corresponding percentage of deaths among the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b5c140-c0e5-45f0-9922-ba04811ef6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 03_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 03_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# Use a dictionary for deaths and cases by state\n",
    "# Initialize with default values 0\n",
    "muertes_por_casos = defaultdict(lambda: [0, 0])\n",
    "\n",
    "# For each line in the STDIN\n",
    "for linea in sys.stdin:\n",
    "    estado, casos, muertes = linea.strip().split('\\t')\n",
    "    muertes_por_casos[estado][0] += int(muertes)\n",
    "    muertes_por_casos[estado][1] += int(casos)\n",
    "\n",
    "# Calculate the state with the highest average number of deaths due to positive cases\n",
    "max_estado, max_media = max(\n",
    "    ((estado, (muertes / casos) if casos != 0 else 0)\n",
    "     for estado, (muertes, casos) in muertes_por_casos.items()),\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "porcentaje = round(max_media * 100, 2)\n",
    "\n",
    "print(f\"El estado con mayor mortandad por casos positivos fue {max_estado}\")\n",
    "print(f\"Hubo un {porcentaje}% de muertes de los casos encontrados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be420d1d-546e-4718-8ab4-81ed3138d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El estado con mayor mortandad por casos positivos fue Puerto Rico\n",
      "Hubo un 35.78% de muertes de los casos encontrados\n"
     ]
    }
   ],
   "source": [
    "# Run our map-reduce and see the result from the console \n",
    "! cat /media/notebooks/covid19/covid19-NY.csv \\\n",
    "| python3 03_mapper.py | sort | python3 03_reducer.py > salida-03\n",
    "\n",
    "! cat salida-03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef7e52-9db8-4ee3-9161-3b3a7813d667",
   "metadata": {},
   "source": [
    "**Hadoop Streaming Execution # 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a975eaf2-b002-4082-a500-0c0eb7e22770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar4306110333602406323/] [] /tmp/streamjob2949191051670331604.jar tmpDir=null\n",
      "2024-08-28 11:52:47,379 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:52:48,143 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:52:49,513 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1724833657149_0032\n",
      "2024-08-28 11:52:50,956 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-08-28 11:52:51,304 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-08-28 11:52:52,038 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1724833657149_0032\n",
      "2024-08-28 11:52:52,038 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-08-28 11:52:52,620 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-08-28 11:52:52,621 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-08-28 11:52:52,877 INFO impl.YarnClientImpl: Submitted application application_1724833657149_0032\n",
      "2024-08-28 11:52:53,013 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1724833657149_0032/\n",
      "2024-08-28 11:52:53,024 INFO mapreduce.Job: Running job: job_1724833657149_0032\n",
      "2024-08-28 11:53:07,745 INFO mapreduce.Job: Job job_1724833657149_0032 running in uber mode : false\n",
      "2024-08-28 11:53:07,747 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-08-28 11:53:35,328 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2024-08-28 11:53:41,934 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2024-08-28 11:53:47,662 INFO mapreduce.Job:  map 83% reduce 0%\n",
      "2024-08-28 11:53:53,949 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-08-28 11:54:20,783 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-08-28 11:54:21,828 INFO mapreduce.Job: Job job_1724833657149_0032 completed successfully\n",
      "2024-08-28 11:54:22,100 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45752396\n",
      "\t\tFILE: Number of bytes written=92447370\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=90204208\n",
      "\t\tHDFS: Number of bytes written=119\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=98428\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18801\n",
      "\t\tTotal time spent by all map tasks (ms)=98428\n",
      "\t\tTotal time spent by all reduce tasks (ms)=18801\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=98428\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=18801\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=100790272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=19252224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2445227\n",
      "\t\tMap output records=2445227\n",
      "\t\tMap output bytes=40861936\n",
      "\t\tMap output materialized bytes=45752402\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=56\n",
      "\t\tReduce shuffle bytes=45752402\n",
      "\t\tReduce input records=2445227\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=4890454\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1263\n",
      "\t\tCPU time spent (ms)=24180\n",
      "\t\tPhysical memory (bytes) snapshot=1116934144\n",
      "\t\tVirtual memory (bytes) snapshot=7636385792\n",
      "\t\tTotal committed heap usage (bytes)=1066926080\n",
      "\t\tPeak Map Physical memory (bytes)=348667904\n",
      "\t\tPeak Map Virtual memory (bytes)=2558304256\n",
      "\t\tPeak Reduce Physical memory (bytes)=436006912\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2549186560\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90204018\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=119\n",
      "2024-08-28 11:54:22,100 INFO streaming.StreamJob: Output directory: /covid19/mapreduce/03_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 03_mapper.py,03_reducer.py \\\n",
    "-mapper 03_mapper.py \\\n",
    "-reducer 03_reducer.py \\\n",
    "-input /covid19/covid19-NY.csv \\\n",
    "-output /covid19/mapreduce/03_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3f40f2-461e-4827-8bf2-7552c67db1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El estado con mayor mortandad por casos positivos fue Puerto Rico\t\n",
      "Hubo un 35.78% de muertes de los casos encontrados\t\n",
      "\n",
      "Salida del dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>deaths</th>\n",
       "      <th>cases</th>\n",
       "      <th>muertes-casos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>1605636</td>\n",
       "      <td>4488011</td>\n",
       "      <td>35.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state   deaths    cases  muertes-casos\n",
       "42  Puerto Rico  1605636  4488011          35.78"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See exercise 3 output\n",
    "! hdfs dfs -cat /covid19/mapreduce/03_salida/part-00000 \n",
    "\n",
    "# Check in the dataframe the state with the most deaths by case\n",
    "df_agrupado = dataframe[['state', 'deaths', 'cases']].groupby('state').sum().reset_index()\n",
    "\n",
    "# Create a new column with the percentage division\n",
    "df_agrupado['muertes-casos'] = (( df_agrupado['deaths'] / df_agrupado['cases'] ) * 100).round(2)\n",
    "max_deaths_cases = df_agrupado[df_agrupado['muertes-casos'] == df_agrupado['muertes-casos'].max()]\n",
    "print(\"\\nSalida del dataframe:\\n\")\n",
    "max_deaths_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2743661-f040-4b29-9586-72860d330f47",
   "metadata": {},
   "source": [
    "## Exercise 4 - Ranked count. Number of counties analyzed by state\n",
    "\n",
    "We can also perform range counting using MapReduce. In this exercise we will count for each state the unique counties that have been analyzed. We will need the state and county field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0fcdd0-b845-445d-afa5-9ebffe67a820",
   "metadata": {},
   "source": [
    "**MAP Process # 04**\n",
    "\n",
    "Processes input data line by line, extracting the state and county information from each record. It splits each line into columns, selects the relevant columns (state and county), and then outputs a key-value pair where the key is the state and the value is the county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edf423c2-dd0c-4184-8e98-aa6a91a00afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 04_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 04_mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# STDIN\n",
    "for linea in sys.stdin:\n",
    "  fila = linea.strip()\n",
    "  campos = fila.split(',')\n",
    "\n",
    "  estado = campos[2]\n",
    "  condado = campos[1]\n",
    "    \n",
    "  print(f\"{estado}\\t{condado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a34c86-f236-4711-b363-9b5daed664ae",
   "metadata": {},
   "source": [
    "**REDUCE Process # 04**\n",
    "\n",
    "Processes the input data by reading each line containing a state and county pair. It stores this information in a dictionary, where the key is the state and the value is a list of counties associated with that state. After processing all the input, it filters the counties to count the unique counties for each state using a set. Finally, it prints the number of unique counties for each state, indicating how many distinct counties are present in the data for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f645da80-ac41-43a5-b7d4-ed19a0ddf571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 04_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 04_reducer.py\n",
    "#!/usr/bin/env python3\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "# dictionary for state and counties\n",
    "estados_totales = defaultdict(list)\n",
    "\n",
    "# For each line read the state and county\n",
    "for linea in sys.stdin:\n",
    "    estado, condado = linea.strip().split('\\t')\n",
    "\n",
    "    # Append the county to your state list\n",
    "    estados_totales[estado].append(condado)\n",
    "\n",
    "for estado, condados_totales in estados_totales.items():\n",
    "    # With a set object we filter out the unique counties and count them\n",
    "    num_condados = len(set(condados_totales))\n",
    "\n",
    "    # Total number of counties analyzed by state\n",
    "    print(f\"{num_condados} condados - Estado de {estado}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "875f3ea9-3361-4fda-9bda-d1eab4c1cc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 condados - Estado de Alabama\n",
      "28 condados - Estado de Alaska\n",
      "1 condados - Estado de American Samoa\n",
      "16 condados - Estado de Arizona\n",
      "76 condados - Estado de Arkansas\n",
      "59 condados - Estado de California\n",
      "65 condados - Estado de Colorado\n",
      "9 condados - Estado de Connecticut\n",
      "4 condados - Estado de Delaware\n",
      "1 condados - Estado de District of Columbia\n"
     ]
    }
   ],
   "source": [
    "# Run our map-reduce and see the result in the console \n",
    "! cat /media/notebooks/covid19/covid19-NY.csv \\\n",
    "| python3 04_mapper.py | sort | python3 04_reducer.py > salida-04\n",
    "\n",
    "! cat salida-04 | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07fc52-eb45-4805-af27-f8114a9ff040",
   "metadata": {},
   "source": [
    "**Hadoop Streaming Execution # 04**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6edf33a-b3b8-4bbb-a11a-84f2380fc742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar8960482822706783748/] [] /tmp/streamjob2748713371978554974.jar tmpDir=null\n",
      "2024-08-28 11:54:41,883 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:54:42,279 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at yarnmanager/172.18.0.7:8032\n",
      "2024-08-28 11:54:43,146 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1724833657149_0033\n",
      "2024-08-28 11:54:45,211 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-08-28 11:54:45,791 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2024-08-28 11:54:46,841 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1724833657149_0033\n",
      "2024-08-28 11:54:46,842 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-08-28 11:54:47,338 INFO conf.Configuration: resource-types.xml not found\n",
      "2024-08-28 11:54:47,339 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2024-08-28 11:54:47,517 INFO impl.YarnClientImpl: Submitted application application_1724833657149_0033\n",
      "2024-08-28 11:54:47,604 INFO mapreduce.Job: The url to track the job: http://yarnmanager:8088/proxy/application_1724833657149_0033/\n",
      "2024-08-28 11:54:47,608 INFO mapreduce.Job: Running job: job_1724833657149_0033\n",
      "2024-08-28 11:55:00,432 INFO mapreduce.Job: Job job_1724833657149_0033 running in uber mode : false\n",
      "2024-08-28 11:55:00,433 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2024-08-28 11:55:23,142 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2024-08-28 11:55:37,612 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-08-28 11:55:38,648 INFO mapreduce.Job: Job job_1724833657149_0033 completed successfully\n",
      "2024-08-28 11:55:38,857 INFO mapreduce.Job: Counters: 55\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=47165971\n",
      "\t\tFILE: Number of bytes written=95274520\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=90204208\n",
      "\t\tHDFS: Number of bytes written=1964\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=39989\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12318\n",
      "\t\tTotal time spent by all map tasks (ms)=39989\n",
      "\t\tTotal time spent by all reduce tasks (ms)=12318\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=39989\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=12318\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=40948736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=12613632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2445227\n",
      "\t\tMap output records=2445227\n",
      "\t\tMap output bytes=42275511\n",
      "\t\tMap output materialized bytes=47165977\n",
      "\t\tInput split bytes=190\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=56\n",
      "\t\tReduce shuffle bytes=47165977\n",
      "\t\tReduce input records=2445227\n",
      "\t\tReduce output records=56\n",
      "\t\tSpilled Records=4890454\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=1151\n",
      "\t\tCPU time spent (ms)=13920\n",
      "\t\tPhysical memory (bytes) snapshot=1048293376\n",
      "\t\tVirtual memory (bytes) snapshot=7632457728\n",
      "\t\tTotal committed heap usage (bytes)=888668160\n",
      "\t\tPeak Map Physical memory (bytes)=360660992\n",
      "\t\tPeak Map Virtual memory (bytes)=2541776896\n",
      "\t\tPeak Reduce Physical memory (bytes)=330678272\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2549747712\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90204018\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1964\n",
      "2024-08-28 11:55:38,858 INFO streaming.StreamJob: Output directory: /covid19/mapreduce/04_salida\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.4.0.jar \\\n",
    "-files 04_mapper.py,04_reducer.py \\\n",
    "-mapper 04_mapper.py \\\n",
    "-reducer 04_reducer.py \\\n",
    "-input /covid19/covid19-NY.csv \\\n",
    "-output /covid19/mapreduce/04_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99323069-aafa-444a-8627-f969ed0740ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 condados - Estado de Alabama\t\n",
      "28 condados - Estado de Alaska\t\n",
      "1 condados - Estado de American Samoa\t\n",
      "16 condados - Estado de Arizona\t\n",
      "76 condados - Estado de Arkansas\t\n",
      "59 condados - Estado de California\t\n",
      "65 condados - Estado de Colorado\t\n",
      "9 condados - Estado de Connecticut\t\n",
      "4 condados - Estado de Delaware\t\n",
      "1 condados - Estado de District of Columbia\t\n",
      "\n",
      "Salida del dataframe:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Samoa</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorado</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Connecticut</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delaware</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      county\n",
       "state                       \n",
       "Alabama                   67\n",
       "Alaska                    28\n",
       "American Samoa             1\n",
       "Arizona                   16\n",
       "Arkansas                  76\n",
       "California                59\n",
       "Colorado                  65\n",
       "Connecticut                9\n",
       "Delaware                   4\n",
       "District of Columbia       1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See exercise 4 output\n",
    "! hdfs dfs -cat /covid19/mapreduce/04_salida/part-00000 | head -n 10\n",
    "\n",
    "# Count the unique counties for each state\n",
    "df_condados_estados = dataframe[['state', 'county']].groupby('state').nunique()\n",
    "print(\"\\nSalida del dataframe:\\n\")\n",
    "df_condados_estados.sort_values(by='state').head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
